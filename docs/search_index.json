[["index.html", "Regression Analysis Notes Chapter 1 Prerequisites 1.1 Regression Model 1.2 Continuous Random Variables", " Regression Analysis Notes Ziyuan Huang 2025-10-22 Chapter 1 Prerequisites 1.1 Regression Model Given response \\(Y\\in\\mathbb{R}\\) and predictors \\(\\underline{X}=(X_1,\\dots,X_p)\\in\\mathbb{R}^p\\), the goal is to model the relationship between \\(Y\\) and \\(\\underline{X}\\) (called to regress \\(Y\\) onto \\(\\underline{X}\\)) by \\[ Y=f(\\underline{X})+\\varepsilon \\] where \\(f(\\cdot)\\) is unknown and \\(\\varepsilon\\) is the noise. The input is a dataset of size \\(n\\): \\(\\left\\{(x_{ij})_{j\\in[p]},y_i\\right\\}_{i\\in[n]}\\). Linear Regression Model Assume \\(f(X) = \\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p\\). Reduce estimation of functions to estimation of parameters. A linear model is linear in parameters not linear in predictors! Predictors can be transformed features such as \\(\\ln(X_1)\\) or \\(X_1X_2\\) Generalized linear model: \\[g(\\mathbb{E}[Y|X])=\\beta_0+\\sum_{i\\in[p]}\\beta_iX_i\\] for some known function \\(g\\). 1.2 Continuous Random Variables Definition 1.1 (t-distribution) The \\(t\\)-distribution with degrees of freedom \\(n\\) is determined by the following pdf \\[ t_n \\sim f(x) = \\frac{\\Gamma(\\frac{n+1}{2})}{\\sqrt{n\\pi}\\cdot\\Gamma(\\frac{n}{2})}\\left(1+\\frac{x^2}{n}\\right)^{-\\frac{n+1}{2}} \\] where \\(\\Gamma(z) = \\int_0^\\infty t^{z-1} e^{-t}dt\\). \\(t\\)-distribution is symmetric around zero, bell-shaped, but has heavier tails than normal \\(t_n\\to \\mathcal{N}(0,1)\\) as \\(n\\to\\infty\\) Definition 1.2 (Beta distribution) Beta distribution with parameters \\(a&gt;0\\) and \\(b&gt;0\\) has pdf \\[ \\textrm{Beta}(a, b)\\sim f(x)=\\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}x^{a-1}(1-x)^{b-1},\\quad 0&lt;x&lt;1 . \\] The inverse coefficient is also called the beta-function \\(B(a,b)=\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}\\) Beta distribution is a generalization of uniform distribution to distributions over \\([0,1]\\) \\(\\textrm{Beta}(1,1)=\\textrm{Unif}(0,1)\\) U-shaped when \\(a&lt;1\\) and \\(b&lt;1\\); and is bell-shaped when \\(a&gt;1\\) and \\(b&gt;1\\) Skewed towards 0 when \\(a&lt;b\\); and is skewed towards 1 when \\(a&gt;b\\). Theorem 1.1 (Beta Conjugate) Beta distribution is the conjugate distribution for binomial distribution. If prior is \\(p\\sim\\textrm{Unif}(0,1)\\), let \\(X\\sim\\textrm{Bin}(n,p)\\), then \\[ \\mathbb{P}(X=k) = \\int\\begin{pmatrix}n \\\\ k\\end{pmatrix}x^k(1-x)^{n-k}dx = \\frac{1}{n+1} \\] meaning the expected number of successes are uniformly distributed. Let \\(p\\sim\\textrm{Beta}(a,b)\\) be the prior and the number of successes be \\(k\\). Then, the posterior is \\(\\textrm{Beta}(a+k,b+n-k)\\). Thus, \\(a-1\\) represents the number of prior successes and \\(b-1\\) represents the number of prior failures. Theorem 1.2 (Beta Order Statistic) Let \\(X\\) be the distribution of the \\(k\\)-th smallest element of \\(n\\) uniform and independent tosses Then, \\(X\\sim\\textrm{Beta}(k,n-k)\\). Proof. Observe \\(\\mathbb{P}(X\\leq t) = \\mathbb{P}(\\textrm{at least k tosses land in [0,t]})=\\sum_{i\\geq k}\\begin{pmatrix}n\\\\i\\end{pmatrix}t^i(1-t)^{n-i}\\). Thus, \\[ \\begin{aligned} f_{X}(t) &amp;= \\sum_{i\\geq k}\\begin{pmatrix}n\\\\i\\end{pmatrix}\\left(it^{i-1}(1-t)^{n-i} - (n-i)t^i(1-t)^{n-i-1}\\right) \\\\ &amp;= \\begin{pmatrix}n \\\\ k\\end{pmatrix}kt^{k-1}(1-t)^{n-k} + \\sum_{i=k}^{n-1}\\left[-(n-i)\\begin{pmatrix}n \\\\ i\\end{pmatrix}+(i+1)\\begin{pmatrix}n \\\\ i+1\\end{pmatrix}\\right] t^i(1-t)^{n-i-1} \\\\ &amp; = \\frac{n!}{(k-1)!(n-k)!}t^{k-1}(1-t)^{n-k} \\sim \\textrm{Beta}(k, n-k+1) \\end{aligned} \\] Notice that for integer \\(n\\), \\(\\Gamma(n)=(n-1)!\\). Definition 1.3 (Gamma distribution) Gamma distribution with shape parameter \\(\\alpha\\) and scale parameter \\(\\theta\\) has pdf \\[ \\textrm{Gamma}(\\alpha,\\theta) \\sim f(x;\\alpha,\\theta) = \\frac{1}{\\Gamma(\\alpha)\\theta^\\alpha}x^{\\alpha - 1} e^{-\\frac{x}{\\theta}},\\quad 0\\leq x&lt;\\infty. \\] Lemma 1.1 (Infinite divisibility) Let \\(X_i\\sim\\textrm{Gamma}(\\alpha_i,\\theta)\\) for \\(i\\in[n]\\) be independent, then \\[ \\sum_{i=1}^nX_i \\sim \\textrm{Gamma}\\left(\\sum_{i\\in[n]}\\alpha_i,\\theta\\right). \\] This implies that Gamma distribution is additive in the shape parameter and that it can be decomposed as the sum of arbitrary independent random variables — a property called infinite divisibility. Proof. It is sufficient to show the additivity property for \\(X_1\\) and \\(X_2\\). \\[ \\begin{aligned} f_{X_1+X_2}(t) &amp;= \\int_{0}^\\infty f_{X_1}(x)f_{X_2}(t-x)dx \\\\ &amp;= \\int_{0}^t\\frac{x^{\\alpha_1-1}e^{-x/\\theta}}{\\Gamma(\\alpha_1)\\theta^{\\alpha_1}}\\frac{(t-x)^{\\alpha_2-1}e^{-(t-x)/\\theta}}{\\Gamma(\\alpha_1)\\theta^{\\alpha_2}}dx \\\\ &amp;=e^{-t/\\theta}\\int_0^t \\frac{x^{\\alpha_1-1}(t-x)^{\\alpha_2-1}}{\\Gamma(\\alpha_1)\\Gamma(\\alpha_2)\\theta^{\\alpha_1+\\alpha_2}}dx \\\\ &amp;\\overset{x:=tz}{=}\\frac{t^{\\alpha_1+\\alpha_2-1}e^{-t/\\theta}}{\\Gamma(\\alpha_1+\\alpha_2)\\theta^{\\alpha_1+\\alpha_2}}\\int_0^1\\frac{\\Gamma(\\alpha_1+\\alpha_2)}{\\Gamma(\\alpha_1)\\Gamma(\\alpha_2)}z^{\\alpha_1-1}(1-z)^{\\alpha_2-1}dz \\\\ &amp;=\\frac{t^{\\alpha_1+\\alpha_2-1}e^{-t/\\theta}}{\\Gamma(\\alpha_1+\\alpha_2)\\theta^{\\alpha_1+\\alpha_2}} \\sim \\textrm{Gamma}(\\alpha_1+\\alpha_2,\\theta) \\end{aligned} \\] where the last equality used the fact that \\(\\textrm{Beta}(\\alpha_1,\\alpha_2)\\sim \\frac{\\Gamma(\\alpha_1+\\alpha_2)}{\\Gamma(\\alpha_1)\\Gamma(\\alpha_2)}z^{\\alpha_1-1}(1-z)^{\\alpha_2-1}\\). Remarks \\(\\textrm{Gamma}(1,\\theta)=\\textrm{Exp}(1/\\theta)\\) and \\(\\textrm{Gamma}(n,\\theta)=\\sum_{i=1}^n\\textrm{Exp}(1/\\theta)\\): \\(\\textrm{Gamma}(n,\\theta)\\) models the waiting time for the next \\(n\\) independent events to occur, compared to the exponential distribution that only models the very next event. Scaling property: \\(X\\sim\\textrm{Gamma}(\\alpha,\\theta)\\implies cX\\sim\\textrm{Gamma}(\\alpha,c\\theta)\\) Theorem 1.3 (Beta and Gamma) Let \\(X\\sim\\textrm{Gamma}(\\alpha,\\theta)\\) and \\(Y\\sim\\textrm{Gamma}(\\beta,\\theta)\\) be independent. Then, \\(\\frac{X}{X+Y}\\sim\\textrm{Beta}(\\alpha,\\beta)\\) and \\(\\frac{X}{X+Y}\\) is independent of \\(X+Y\\). Proof. We apply the transformation rule: \\(f_Y(y)=f_X(x)\\left|\\frac{dx}{dy}\\right|\\). In this case, \\(x=(X,Y)\\) and \\(y=(\\frac{X}{X+Y}, X+Y)\\). The mapping \\(y\\mapsto x\\) is \\(x_1=y_1y_2\\) and \\(x_2=(1-y_1)y_2\\). \\[ \\left|\\frac{dx}{dy}\\right| = \\left|\\begin{pmatrix} y_2 &amp; y_1 \\\\ -y_2 &amp; 1-y_1 \\end{pmatrix}\\right| = \\left|\\begin{pmatrix} X+Y &amp; \\frac{X}{X+Y} \\\\ -X-Y &amp; \\frac{Y}{X+Y} \\end{pmatrix}\\right| = Y + X. \\] Thus, the joint distribution of \\((\\frac{X}{X+Y}, X+Y)\\) equals \\[ \\begin{aligned} f_{\\frac{X}{X+Y}, X+Y}(w,t) &amp;= f_X(wt)f_Y((1-w)t)t \\\\ &amp;\\propto t(wt)^{\\alpha-1}e^{-wt/\\theta}((1-w)t)^{\\beta-1}e^{-(1-w)t/\\theta} \\\\ &amp;= \\underbrace{w^{\\alpha-1}(1-w)^{\\beta-1}}_{\\sim\\textrm{Beta}(\\alpha,\\beta)}\\cdot \\underbrace{t^{\\alpha+\\beta-1}e^{-t/\\theta}}_{\\sim\\textrm{Gamma}(\\alpha+\\beta,\\theta)} \\end{aligned} \\] The separability of the two pdfs shows the independence. Definition 1.4 (Chi-squared distribution) A chi-square distribution with degrees of freedom \\(n\\) is the distribution of a sum of \\(n\\) independent standard normal random variables, denoted as \\(\\chi_n^2\\). \\(\\chi_n^2\\) distribution is a special case of the gamma distribution \\(\\chi_n^2=\\textrm{Gamma}\\left(\\frac{n}{2},2\\right)\\) Theorem 1.4 (Normal over Chi is t-distribution) Let \\(Z\\sim\\mathcal{N}(0,1)\\) and \\(W\\sim\\chi_n^2\\). Then, \\(\\frac{Z}{\\sqrt{W/n}}\\sim t_n\\). Proof. We first calculate the distribution of \\(\\sqrt{W/n}\\), using the scaling property of the gamma distribution: \\[ \\begin{aligned} \\mathbb{P}(\\sqrt{W/n}\\leq t) &amp;= \\mathbb{P}(W/n\\leq t^2) = \\mathbb{P}\\left(\\textrm{Gamma}\\left(\\frac{n}{2},\\frac{2}{n}\\right)\\leq t^2\\right) \\end{aligned} \\] Using the chain rule, we have \\[ f_{\\sqrt{W/n}}(x) = 2t\\left(\\frac{n}{2}\\right)^{\\frac{n}{2}}\\cdot\\frac{x^{n-2}e^{-\\frac{nx^2}{2}}}{\\Gamma(\\frac{n}{2})} = \\frac{n^{n/2}}{2^{n/2-1}}\\frac{x^{n-1}e^{-\\frac{nx^2}{2}}}{\\Gamma(\\frac{n}{2})} \\] Define \\(T:=Z/\\sqrt{W/n}\\). Then, use the ratio distribution formula for independent random variables: \\[ \\begin{aligned} f_T(t) &amp;= \\int_{-\\infty}^\\infty|x|f_Z(tx)f_{\\sqrt{W/n}}(x)dx = \\frac{1}{\\sqrt{2\\pi}}\\frac{n^{n/2}}{2^{n/2-1}\\Gamma(\\frac{n}{2})}\\int_{0}^\\infty|x| x^{n-1}\\exp\\left(-\\frac{t^2x^2}{2}-\\frac{n}{2}x^2\\right)dx \\\\ &amp;=\\frac{1}{\\sqrt{2\\pi}}\\frac{n^{n/2}}{2^{n/2-1}\\Gamma(\\frac{n}{2})}\\int_{0}^\\infty x^{n}\\exp\\left(-\\frac{t^2+n}{2}x^2\\right)dx \\\\ &amp;\\overset{z:=x^2}{=}\\frac{1}{\\sqrt{2\\pi}}\\frac{n^{n/2}}{2^{n/2-1}\\Gamma(\\frac{n}{2})}\\frac{1}{2}\\int_{0}^\\infty z^{\\frac{n-1}{2}}\\exp\\left(-\\frac{t^2+n}{2}z\\right)dz \\\\ &amp;=\\frac{1}{\\sqrt{2\\pi}}\\frac{n^{n/2}}{2^{n/2}\\Gamma(\\frac{n}{2})}\\Gamma\\left(\\frac{n+1}{2}\\right)\\left(\\frac{t^2+n}{2}\\right)^{-\\frac{n+1}{2}}\\int_{0}^\\infty f_{\\textrm{Gamma}\\left(\\frac{n+1}{2},\\frac{2}{t^2+n}\\right)}(z)dz \\\\ &amp;= \\frac{\\Gamma(\\frac{n+1}{2})}{\\sqrt{\\pi n}\\Gamma(\\frac{n}{2})}(1+t^2/n)^{-\\frac{n+1}{2}} \\end{aligned} \\] This is exactly the pdf of \\(t_n\\). Definition 1.5 (F-distribution) Let \\(U\\sim\\chi_m^2\\), \\(v\\sim\\chi_n^2\\), and \\(U,V\\) be independent. Then, \\(\\frac{U/m}{V/n}\\sim F_{m,n}\\), an \\(F\\)-distribution with \\(m\\) and \\(n\\) degrees of freedom. "],["estimation.html", "Chapter 2 Estimation 2.1 Simple Linear Regression 2.2 Multiple Linear Regression 2.3 Error Analysis 2.4 The Gauss-Markov Theorem 2.5 Introducing Predictors 2.6 R Example", " Chapter 2 Estimation Estimator is a statistic (a function of the observed finite samples/dataset) that is used to estimate an unknown population parameter. Criterion: Least Squares (LS) or Sum of Squared Errors (SSE) \\[ \\min_{\\boldsymbol{\\beta}\\in\\mathbb{R}^{p+1}}\\;\\sum_{i\\in[n]}\\varepsilon_i^2=\\sum_{i\\in[n]}\\left(y_i-\\beta_0-\\sum_{j\\in[p]}x_{ij}\\beta_j\\right)^2. \\] 2.1 Simple Linear Regression This is the special case when \\(p=1\\) (i.e., one predictor). The optimization problem is convex and can be solved with the first order condition: \\[ \\hat{\\beta}_1=\\frac{\\sum_{i\\in[n]}(x_{i}-\\bar{x})(y_i-\\bar{y})}{\\sum_{i\\in[n]}(x_i-\\bar{x})^2}=\\frac{s^2_{xy}}{s_x^2}=r_{xy}\\times\\frac{s_y}{s_x} \\] where \\(s^2_{xy}\\), \\(s^2_x\\), \\(s_y^2\\), and \\(r_{xy}\\) are sample covariance of \\(X\\) and \\(Y\\), sample variance of \\(X\\), sample variance of \\(Y\\), and sample correlation of \\(X\\) and \\(Y\\), respectively; and \\[ \\hat{\\beta}_0=\\bar{y}-\\hat{\\beta}_1\\bar{x}. \\] Plugin the above solution into the regression line \\(y=\\hat{\\beta}_1x+\\hat{\\beta}_0\\), we obtain the concise expression \\[ \\frac{y-\\bar{y}}{s_y}=r\\frac{x-\\bar{x}}{s_x} \\] Remarks Let x and y be standardized by sample mean and standard deviation. Then, the regression lines are \\(y=rx\\) and \\(x=ry\\). Notice that the lines are different unless \\(r=\\pm 1\\), representing perfect correlation. 2.2 Multiple Linear Regression This is the general case with \\(p\\geq 1\\). Denote \\(\\boldsymbol{x}_i=(1, x_{i1},\\dots,x_{ip})^\\top\\in\\mathbb{R}^{p+1}\\) and the data matrix \\(X:=(\\boldsymbol{x}_1,\\dots,\\boldsymbol{x}_n)^\\top\\in\\mathbb{R}^{(p+1)\\times n}\\). The estimator is given by the MSE solution \\[ \\hat{\\boldsymbol{\\beta}} = (X^\\top X)^{-1}X^\\top\\boldsymbol{y} \\] Remarks \\(H:=X(X^\\top X)^{-1}X\\) is the projection matrix onto \\(\\textrm{Col}(X)\\). In other words, linear regression is equivalent to projecting the response vector on to \\(\\textrm{Col}(X)\\) and \\(\\hat{\\boldsymbol{\\beta}}\\) is the projection coefficient. \\(H\\) is also called the hat matrix as it converts the response vector \\(\\boldsymbol{y}\\) to the “hatted” (i.e., predicted) response vector \\(\\hat{\\boldsymbol{y}}\\) \\(R:=I-H\\) is the residual matrix as it produces the residual vector \\(\\hat{\\boldsymbol{e}}:=\\boldsymbol{y}-\\hat{\\boldsymbol{y}}\\) \\(\\hat{\\boldsymbol{\\beta}}\\) is unbiased: \\(\\mathbb{E}[\\hat{\\boldsymbol{\\beta}}] = (X^\\top X)^{-1}X^\\top(X\\vec{\\beta}+\\varepsilon)=\\boldsymbol{\\beta}\\) When noise are homoscedasticity with variance \\(\\sigma^2\\), then the variance of the estimator equals \\[\\textrm{Var}(\\hat{\\boldsymbol{\\beta}})=(X^\\top X)^{-1}X^\\top\\textrm{Var}(\\varepsilon)X(X^\\top X)^{-1}=\\sigma^2(X^\\top X)^{-1}\\] An unbiased estimator of the noise variance is \\[ \\hat{\\sigma}^2=\\frac{\\sum_{i\\in[n]}(y_i-\\hat{y})^2}{n-(p+1)}. \\] 2.3 Error Analysis Error analysis is a study of the residuals \\(e_i:=y_i-\\hat{y}_i\\) for all \\(i\\in[n]\\). Lemma 2.1 \\(\\sum_{i\\in[n]}e_i=0\\) or equivalently \\(\\frac{1}{n}\\sum_{i\\in[n]}\\hat{y}_i=\\bar{y}\\). Proof. By the projection theorem, \\(\\boldsymbol{e}\\perp \\textrm{Col}(X)\\). Then, notice \\(\\boldsymbol{1}\\in\\textrm{Col}(X)\\). Define residual sum of squares (i.e., the LS loss at the optimal estimator) as \\[ RSS :=\\sum_{i\\in[n]}e_i^2 = \\sum_{i\\in[n]}(y_i-\\hat{y}_i)^2 \\] and the total sum of squares \\[ TSS := \\sum_{i\\in[n]}(y_i-\\bar{y}_i)^2. \\] We measure the goodness-of-fit using the coefficient of determination or \\(R^2\\) \\[ R^2=1-\\frac{RSS}{TSS} \\] \\(R^2\\) measures the proportion of variability of the response variable that can be explained by the predictors. Remarks RSS alone as the error metric has the problem that it does not have reasonable units. Variability of the predictors or regression sum of squares: \\(\\sum_{i\\in[n]}(\\hat{y}_i-\\bar{y})^2=TSS - RSS\\) For simple linear regression: \\(R^2 = r^2\\) Small \\(R^2\\) does not mean \\(x,y\\) are not linearly related; it just means the variance of noise is high compared to the trend (i.e., low snr) Large \\(R^2\\) does not mean the model is correct (e.g., quadratic model with zero noise) \\(R^2\\) always increase when adding more predictors to the model because setting \\(\\beta\\) for the new predictor to zero recovers the old \\(RSS\\), which equals to the LS under the optimal estimator. Adjusted \\(R^2\\) is used to mitigate the issue of number of predictors: \\[ \\textrm{Adjusted }R^2 = 1 - \\frac{RSS/(n-p-1)}{TSS / (n-1)} = 1-\\frac{\\hat{\\sigma}^2}{s_y^2} \\] 2.4 The Gauss-Markov Theorem Definition 2.1 A linear estimator of the quantity \\(\\psi\\) is a linear combination of the responses: \\(\\hat{\\psi}=c_1y_1+\\dots+c_ny_n\\). The linear regression produces the best linear unbiased estimator (BLUE) in the following sense. Theorem 2.1 Suppose \\(\\boldsymbol{y}=X\\boldsymbol{\\beta}+\\varepsilon\\), \\(X\\) is full rank, \\(\\mathbb{E}[\\varepsilon]=\\boldsymbol{0}\\), and \\(\\textrm{Var}(\\varepsilon)=\\sigma^2I\\). Consider the quantity \\(\\psi=\\boldsymbol{c}^\\top\\boldsymbol{\\beta}\\). Then, among all unbiased and linear estimator of \\(\\psi\\), \\(\\hat{\\psi}:=\\boldsymbol{c}^\\top\\hat{\\boldsymbol{\\beta}}\\) has the minimum variance and is unique. Proof. Let \\(\\tilde{\\psi}=\\boldsymbol{b}^\\top\\boldsymbol{y}\\) be an arbitrary unbiased linear estimator of \\(\\psi\\). Then, we have \\[ \\textrm{Var}(\\tilde{\\psi}) = \\boldsymbol{b}^\\top\\textrm{Var}(\\boldsymbol{y})\\boldsymbol{b} = \\sigma^2\\lVert\\boldsymbol{b}\\rVert^2 \\] Since \\(\\tilde{\\psi}\\) is unbiased, \\(\\mathbb{E}[\\tilde{\\psi}]=\\boldsymbol{b}^\\top X\\boldsymbol{\\beta}=\\boldsymbol{c}^\\top\\boldsymbol{\\beta}\\) for every \\(\\boldsymbol{\\beta}\\). Thus, we must have \\(\\boldsymbol{b}^\\top X=\\boldsymbol{c}^\\top\\). The minimum variance of \\(\\tilde{\\psi}\\) can be obtained from the following optimization problem \\[ \\min_{\\boldsymbol{b}\\in\\mathbb{R}^n}\\;\\lVert\\boldsymbol{b}\\rVert^2\\quad \\textrm{s.t.}\\quad\\boldsymbol{b}^\\top X=\\boldsymbol{c}^\\top. \\] The lagrangian of this problem is \\(L(\\boldsymbol{\\beta},\\boldsymbol{\\lambda})=\\lVert\\boldsymbol{b}\\rVert^2+\\boldsymbol{\\lambda}^\\top(X^\\top\\boldsymbol{b}-\\boldsymbol{c})\\). The first order condition implies \\(\\boldsymbol{b}^*=-\\frac{1}{2}X \\boldsymbol{\\lambda}\\) and the dual function is \\[ D(\\lambda)=-\\frac{1}{4}\\boldsymbol{\\lambda}^\\top(X^\\top X)\\boldsymbol{\\lambda}-\\boldsymbol{\\lambda}^\\top\\boldsymbol{c}. \\] It is easy to check that the strong duality holds and the dual maximizer satisfies \\(\\boldsymbol{\\lambda}^*=-2(X^\\top X)^{-1}\\boldsymbol{c}\\). Thus, we must have \\(\\boldsymbol{b}^*=X(X^\\top X)^{-1}\\boldsymbol{c}\\). So, this minimum variance unbiased linear estimator is \\[ \\hat{\\psi} = (\\boldsymbol{b}^*)^\\top\\boldsymbol{y} = \\boldsymbol{c}^\\top(X^\\top X)^{-1}X^\\top\\boldsymbol{y} = \\boldsymbol{c}^\\top\\hat{\\boldsymbol{\\beta}}. \\] The uniqueness is obvious as the optimization is strongly convex. Remarks In the linear regression model, \\(f(X)\\) is best estimated by \\(X\\hat{\\beta}\\) by taking \\(\\boldsymbol{c}=(1,X_1,\\dots,X_p)^\\top\\) Unbiased cannot be dropped: ridge regression has lower variance than linear regression 2.5 Introducing Predictors 2.5.1 General Theory Let \\(X\\in\\mathbb{R}^{n\\times(p+1)}\\) be existing data matrix and \\(Z\\in\\mathbb{R}^{n\\times q}\\) be the data matrix consisting additional predictors. The linear regression model can be written as \\[ \\mathbb{E}[Y|X,Z] = X\\boldsymbol{\\delta} + Z\\boldsymbol{\\gamma} \\] Let the optimal (LS-minimal) coefficients be \\(\\hat{\\boldsymbol{\\delta}}\\) and \\(\\hat{\\boldsymbol{\\gamma}}\\). Our goal is to compare \\(\\hat{\\boldsymbol{\\delta}}\\) with \\(\\hat{\\boldsymbol{\\beta}}\\) — the regression coefficients with the old predictors only. Theorem 2.2 (Coefficients of Additional Predictors) Assume columns of \\(X\\) and \\(Z\\) are independent so that a unique solution exists. Then, \\(\\hat{\\boldsymbol{\\gamma}}=(Z^\\top RZ)^{-1}Z^\\top R\\boldsymbol{y}\\) — projection coefficient of the response vector onto the \\(X\\)-residualized column space of \\(Z\\) (i.e., \\(\\mathcal{C}(X)^\\perp\\cap\\mathcal{C}(Z)\\)). \\(\\hat{\\boldsymbol{\\delta}}=(X^\\top X)^{-1}X^\\top(\\boldsymbol{y}-Z\\hat{\\boldsymbol{\\gamma}})=\\hat{\\boldsymbol{\\beta}}-(X^\\top X)^{-1}X^\\top Z\\hat{\\boldsymbol{\\gamma}}\\) — The coefficients of the new predictors are determined by the innovations in the new predictors independently, while the coefficients of the existing predictors should bear the linearly dependent component brought by the new predictors. Proof. Rewrite the model as \\(\\mathbb{E}[Y|X,Z] = X\\boldsymbol{\\delta} + HZ\\boldsymbol{\\gamma} + (I-H)Z\\boldsymbol{\\gamma} = (X+HZ)\\tilde{\\boldsymbol{\\delta}}+(I-H)Z\\boldsymbol{\\gamma}\\). Then, it follows \\[ \\begin{aligned} \\begin{pmatrix} \\hat{\\tilde{\\boldsymbol{\\delta}}} \\\\ \\hat{\\boldsymbol{\\gamma}} \\end{pmatrix} &amp;= \\left(\\begin{pmatrix} (X + HZ)^\\top \\\\ Z^\\top(I-H) \\end{pmatrix} \\begin{pmatrix} X + HZ &amp; (I-H)Z \\end{pmatrix}\\right)^{-1} \\begin{pmatrix} (X + HZ)^\\top \\\\ Z^\\top(I-H) \\end{pmatrix}\\boldsymbol{y} \\\\ &amp;= \\begin{pmatrix} (X + HZ)^\\top(X+HZ) &amp; 0 \\\\ 0 &amp; Z^\\top(I-H)Z \\end{pmatrix}^{-1} \\begin{pmatrix} (X + HZ)^\\top \\\\ Z^\\top(I-H) \\end{pmatrix}\\boldsymbol{y} \\\\ \\implies \\hat{\\boldsymbol{\\gamma}} &amp;= (Z^\\top R Z)^{-1}Z^\\top R\\boldsymbol{y}. \\end{aligned} \\] The second bullet point is obtained by plugging in \\(\\hat{\\boldsymbol{\\gamma}}\\) in which case the problem becomes regressing \\(\\boldsymbol{y}-Z\\hat{\\boldsymbol{\\gamma}}\\) onto \\(X\\). Due to symmetry, \\(\\hat{\\boldsymbol{\\delta}}\\) can also be viewed as an independent regression coefficients on the innovations in \\(X\\) against \\(Z\\) Lemma 2.2 (Subspace relationships) Let \\(H_X\\), \\(H_Z\\), and \\(H_{XZ}\\) be the hat matrix when the predictors are \\(X\\), \\(Z\\), and both \\(X\\) and \\(Z\\), respectively. Similarly, we use \\(R_X\\), \\(R_Z\\), and \\(R_{XZ}\\) to denote the corresponding orthogonal matrices. \\(H_{XZ}\\) is the projection onto \\(\\mathcal{C}(X)\\oplus \\mathcal{C}(Z)\\), while \\(R_{XZ}\\) is the projection onto \\(\\mathcal{C}(X)\\cap \\mathcal{C}(Z)\\) \\(R_{XZ}=R_XR_Z\\) and \\(H_{XZ} = 1- R_{XZ}\\) Theorem 2.3 (Residuals of additional predictors) \\(R_{XZ}\\boldsymbol{y}=R_X(\\boldsymbol{y}-Z\\hat{\\boldsymbol{\\gamma}})=R_{Z}(\\boldsymbol{y}-X\\hat{\\boldsymbol{\\delta}})\\) \\(\\textrm{RSS} = (\\boldsymbol{y}-Z\\hat{\\boldsymbol{\\gamma}})^\\top R_X(\\boldsymbol{y}-Z\\hat{\\boldsymbol{\\gamma}})=\\boldsymbol{y}^\\top R_X\\boldsymbol{y}-\\hat{\\boldsymbol{\\gamma}}^\\top Z^\\top R_X\\boldsymbol{y}\\) Proof. The second bullet point follows from \\(R_X\\boldsymbol{y}-R_XZ\\hat{\\boldsymbol{\\gamma}}\\perp R_X\\boldsymbol{y}\\) as projecting \\(\\boldsymbol{y}\\) onto the \\(X\\)-residualized \\(Z\\) is the same as projecting the \\(X\\)-residualized \\(\\boldsymbol{y}\\) onto \\(X\\)-residualized \\(Z\\). 2.6 R Example library(faraway) data(gala) temp = lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, data=gala) summary(temp) ## ## Call: ## lm(formula = Species ~ Area + Elevation + Nearest + Scruz + Adjacent, ## data = gala) ## ## Residuals: ## Min 1Q Median 3Q Max ## -111.679 -34.898 -7.862 33.460 182.584 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.068221 19.154198 0.369 0.715351 ## Area -0.023938 0.022422 -1.068 0.296318 ## Elevation 0.319465 0.053663 5.953 3.82e-06 *** ## Nearest 0.009144 1.054136 0.009 0.993151 ## Scruz -0.240524 0.215402 -1.117 0.275208 ## Adjacent -0.074805 0.017700 -4.226 0.000297 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 60.98 on 24 degrees of freedom ## Multiple R-squared: 0.7658, Adjusted R-squared: 0.7171 ## F-statistic: 15.7 on 5 and 24 DF, p-value: 6.838e-07 Estimate: OLS estimation of \\(\\boldsymbol{\\beta}\\) Std. Error: estimated standard deviation of \\(\\beta_i\\)’s (i.e., \\(\\sqrt{\\hat{\\sigma}^2(X^\\top X)_{ii}^{-1}}\\)) Residual standard error: \\(\\hat{\\sigma}\\) where degrees of freedom is \\(n-p-1\\) "],["inference.html", "Chapter 3 Inference 3.1 Hypothesis Tests 3.2 Confidence Interval (CI)", " Chapter 3 Inference A methodology to draw conclusions about the unknown population parameters \\(\\beta_0,\\dots,\\beta_p\\) from the estimators \\(\\hat{\\beta}_0,\\dots,\\hat{\\beta}_p\\). 3.1 Hypothesis Tests Procedure Consider a null hypothesis \\(H_0\\) and an alternative hypothesis \\(H_A\\). Define p-value as: \\[ \\mathbb{P}(\\textrm{observed or more extreme departure from }H_0\\textrm { in favor of }H_A|H_0\\textrm{ is true}). \\] Given a predetermined significance level \\(\\alpha\\in(0,1)\\), we reject the null hypothesis if p-value \\(&lt;\\alpha\\). Assumption To compute the p-value, we need to make assumptions about the model distribution: \\[ \\varepsilon\\sim \\mathcal{N}_n(\\boldsymbol{0},\\sigma^2I). \\] Then, \\(\\hat{\\boldsymbol{\\beta}}\\sim\\mathcal{N}_p(\\boldsymbol{\\beta}, \\sigma^2(X^\\top X)^{-1})\\) — sample distribution of the statistic \\(\\hat{\\boldsymbol{\\beta}}\\). 3.1.1 T-statistic and F-statistic Based on the normality assumption, we can now derive the distribution of the \\(t\\)-statistic (Theorem 3.1) and the \\(F\\)-statistic (Theorem 3.2) as follows. Definition 3.1 (Standard error) The standard error (SE) of a statistic (usually an estimator of a parameter, like the average or mean) is the standard deviation of its sampling distribution. The standard error of \\(\\hat{\\beta}_i\\) is \\[ \\textrm{se}(\\hat{\\beta}_i) = \\sqrt{\\sigma^2(X^\\top X)^{-1}_{ii}} \\] which can be estimated by (biased) \\[ \\widehat{\\textrm{se}}(\\hat{\\beta}_i) = \\sqrt{\\hat{\\sigma}^2(X^\\top X)^{-1}_{ii}}. \\] Theorem 3.1 (t-statistic) \\(T:=\\frac{\\hat{\\beta_j}-\\beta_j}{\\widehat{\\textrm{se}}(\\hat{\\beta}_j)}\\sim t_{n-(p+1)}\\). This is in contrast to that \\(\\frac{\\hat{\\beta_j}-\\beta_j}{{\\textrm{se}}(\\hat{\\beta}_j)}\\sim \\mathcal{N}(0,1)\\) Proof. It is sufficient to show \\(\\frac{\\widehat{\\textrm{se}}(\\hat{\\beta}_j)}{{\\textrm{se}}(\\hat{\\beta}_j)}=\\sqrt{\\frac{\\hat{\\sigma}^2}{\\sigma^2}}\\sim \\sqrt{\\frac{\\chi^2_{n-p-1}}{n-p-1}}\\). Define the projection matrix \\(\\hat{H}:=I-X(X^\\top X)^{-1}X^\\top\\). So, we have \\[ \\begin{aligned} \\hat{\\sigma}^2 &amp;= \\frac{1}{n-p-1}\\boldsymbol{y}^\\top H\\boldsymbol{y} \\\\ &amp;=\\frac{1}{n-p-1}\\bigg(\\varepsilon^\\top H\\varepsilon + \\underbrace{2\\varepsilon^\\top HX\\boldsymbol{\\beta} + \\boldsymbol{\\beta}^\\top X^\\top H X\\boldsymbol{\\beta}}_{=0\\textrm{ by projection theorem}}\\bigg) \\\\ &amp;=\\frac{1}{n-p-1}\\varepsilon^\\top\\left(\\sum_{i=1}^{n-p-k}\\boldsymbol{u}_i\\boldsymbol{u}_i^\\top\\right)\\varepsilon \\\\ &amp;= \\frac{1}{n-p-1}\\sum_{i=1}^{n-p-1}(\\boldsymbol{u}_i^\\top\\varepsilon)^2 \\end{aligned} \\] where \\(\\boldsymbol{u}_i\\)’s are the eigenvectors of \\(H\\) associated with the \\(n-p-1\\) non-zero eigenvalues. Since \\(\\lVert \\boldsymbol{u}_i\\lVert_2=1\\), we have \\(\\boldsymbol{u}_i^\\top\\varepsilon\\sim\\mathcal{N}(\\boldsymbol{0},\\boldsymbol{u}_i^\\top(\\sigma^2I)\\boldsymbol{u}_i)=\\mathcal{N}(\\boldsymbol{0},\\sigma^2I)\\). Therefore, we have \\((n-p-1)\\hat{\\sigma}^2/\\sigma^2\\sim \\chi_{n-p-1}^2\\). Theorem 3.2 (F-statistic) Suppose \\(H_0\\) claims a subset (\\(\\geq1\\)) of predictors as insignificant (e.g., \\(H_0:p_1=p_2=0\\)) and \\(H_A\\) is the alternative. Fit a model under each of \\(H_0\\) and \\(H_0\\cup H_A\\) and compute \\(RSS_{H_0}\\) and \\(RSS_{H_A}\\) respectively. Then, \\[ F:=\\frac{(\\textrm{RSS}_{H_0}-\\textrm{RSS}_{H_0\\cup H_A})/(\\text{df}_{H_0}-\\text{df}_{H_0\\cup H_A})}{\\textrm{RSS}_{H_0\\cup H_A}/\\textrm{df}_{H_0\\cup H_A}} \\] follows \\(F\\)-distribution with degrees of freedom \\(\\text{df}_{H_0}-\\text{df}_{H_0\\cup H_A}\\) and \\(\\textrm{df}_{H_0\\cup H_A}\\). Proof. Following the proof of Theorem 3.1, we know that \\(\\frac{\\textrm{RSS}_{H_0}}{\\textrm{df}_{H_0}}=\\hat{\\sigma}_{H_0}^2\\sim\\chi_{\\textrm{df}_{H_0}}^2\\). Then, the result follows readily from the additivity of chi-squared distribution and the definition of \\(F\\)-distribution. Here, \\(H_0\\) and \\(H_A\\) should be treated as sets of predictors that are not excluded (due to insignificance). Notice in the \\(F\\)-statistic, the actual alternative hypothesis is \\(H_0\\cup H_A\\), which must be a superset of the null hypothesis (or using more predictors). For example, \\(F\\)-test can test \\(H_0:\\beta_1=\\beta_2=0\\) and \\(H_A:\\beta_1=0\\), but cannot test \\(H_0:\\beta_1=\\beta_2=0\\) and \\(H_A:\\beta_1=\\beta_3=0\\). 3.1.2 Test whether a coefficient \\(\\beta_i\\) is significant \\(H_0:\\beta_i=0\\) and \\(H_A:\\beta_i\\neq 0\\) (or more generally \\(H_0:\\beta_i=c\\) for some constant \\(c\\)) (two-sided) \\(t\\)-test: p-value = \\(\\mathbb{P}(|\\hat{\\beta}_i|\\geq \\hat{\\beta}_i(\\mathcal{D})|H_0) = 1 -\\textrm{CDF}_{t_{n-p-1}}\\left( \\frac{\\hat{\\beta}_i(\\mathcal{D})}{\\widehat{\\textrm{se}}(\\hat{\\beta}_i)}\\right)\\)1 \\(F\\)-test: p-value = \\(1-\\textrm{CDF}_{F_{1,n-p-1}}(\\hat{F}(\\mathcal{D}))\\) where \\(\\hat{F}(\\mathcal{D})\\) is the \\(F\\)-statistic in Theorem 3.2 data(savings) h0 &lt;- lm(sr ~ pop15 + dpi + ddpi, savings) # Model under H0 h0a &lt;- lm(sr ~ pop15 + pop75 + dpi + ddpi, savings) # Model under H0 + HA anova(h0, h0a) ## Analysis of Variance Table ## ## Model 1: sr ~ pop15 + dpi + ddpi ## Model 2: sr ~ pop15 + pop75 + dpi + ddpi ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 46 685.95 ## 2 45 650.71 1 35.236 2.4367 0.1255 For single predictor testing, the \\(F\\)-test and two-sided \\(t\\)-test are equivalent because \\(T^2=F\\) (statistics equal) and \\(t_n=F_{1,n}\\) (distributions equal) By Theorem 2.3, \\[ \\textrm{RSS}_{H_0}-\\textrm{RSS}_{H_0\\cup H_A} = \\hat{\\beta}_i\\boldsymbol{x}_i^\\top \\tilde{R}\\boldsymbol{y} = \\frac{(\\boldsymbol{x}_i^\\top \\tilde{R}\\boldsymbol{y})^2}{\\boldsymbol{x}_i^\\top\\tilde{R}\\boldsymbol{x}_i}=\\hat{\\beta}_i^2\\boldsymbol{x}_i^\\top\\tilde{R}\\boldsymbol{x}_i \\] where \\(\\tilde{R}\\) is the residual matrix of the \\(n-1\\) predictors without \\(\\boldsymbol{x}_i\\). The last piece is completed by \\(\\boldsymbol{x}_i^\\top\\tilde{R}\\boldsymbol{x}_i=(X^\\top X)_{ii}^{-1}\\) using block matrix inversion: \\[ \\begin{aligned} (X^\\top X)^{-1} &amp;= \\begin{pmatrix} \\tilde{X}^\\top \\tilde{X} &amp; \\tilde{X}^\\top\\boldsymbol{x}_i \\\\ \\boldsymbol{x}_i^\\top \\tilde{X} &amp; \\boldsymbol{x}_i^\\top \\boldsymbol{x}_i \\end{pmatrix}^{-1} \\\\ &amp;= \\begin{pmatrix} \\ast &amp; \\ast \\\\ \\ast &amp; \\left(\\boldsymbol{x}_i^\\top\\boldsymbol{x}_i-\\boldsymbol{x}_i^\\top\\tilde{X}(\\tilde{X}^\\top\\tilde{X})^{-1}\\tilde{X}^\\top\\boldsymbol{x}_i\\right)^{-1} \\end{pmatrix} \\\\ &amp;= \\begin{pmatrix} \\ast &amp; \\ast \\\\ \\ast &amp; \\left(\\boldsymbol{x}_i\\tilde{R}\\boldsymbol{x}_i\\right)^{-1} \\end{pmatrix}. \\end{aligned} \\] 3.1.3 Test whether a pair/subset can be excluded from the model \\(H_0:\\beta_1=\\beta_2=0\\) and \\(H_A:\\neg H_0\\) Standard application of \\(F\\)-test Test whether any of the predictors are useful: \\(H_0:\\beta_1=\\dots=\\beta_p=0\\) (not including the intercept).2 data(savings) h0 &lt;- lm(sr ~ pop15 + ddpi, savings) # Model under H0 h0a &lt;- lm(sr ~ pop15 + pop75 + dpi + ddpi, savings) # Model under H0 + HA anova(h0, h0a) ## Analysis of Variance Table ## ## Model 1: sr ~ pop15 + ddpi ## Model 2: sr ~ pop15 + pop75 + dpi + ddpi ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 47 700.55 ## 2 45 650.71 2 49.839 1.7233 0.19 3.1.4 Test whether the pair of predictors has the same effect \\(H_0:\\beta_1=\\beta_2\\) and \\(H_A:\\beta_1\\neq \\beta_2\\) Use \\(F\\)-statistic. Under \\(H_0\\), we have \\(\\mathbb{E}[Y|X] = \\beta_0+\\beta_1(X_1+X_2)+\\beta_3X_3+\\dots+\\beta_pX_p\\). \\(H_A\\) is the original regression line. h0 &lt;- lm(sr ~ I(pop15 + pop75) + dpi + ddpi, savings) h0a &lt;- lm(sr ~ pop15 + pop75 + dpi + ddpi, savings) anova(h0, h0a) ## Analysis of Variance Table ## ## Model 1: sr ~ I(pop15 + pop75) + dpi + ddpi ## Model 2: sr ~ pop15 + pop75 + dpi + ddpi ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 46 673.63 ## 2 45 650.71 1 22.915 1.5847 0.2146 3.2 Confidence Interval (CI) Given the normality assumption of the noise, the CI can be constructed directly from the distributions of \\(\\hat{\\beta}_i\\)’s. Under significance level \\(\\alpha\\) or confidence \\(1-\\alpha\\): Two-sided CI: \\(\\hat{\\beta}_i\\pm t_{n-p-1}^{({\\color{red}\\alpha}{\\color{red}/}{\\color{red}2})}\\widehat{\\textrm{se}}(\\hat{\\beta}_i)\\) One-side CI: \\(\\big(-\\infty,\\hat{\\beta}_i+t_{n-p-1}^{({\\color{red}\\alpha})}\\widehat{\\textrm{se}}(\\hat{\\beta}_i)\\big]\\) Simultaneous CI: notice \\[ \\begin{aligned} &amp;\\frac{(\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta})^\\top\\left[\\widehat{\\textrm{se}}(\\hat{\\boldsymbol{\\beta}})\\right]^{-1}(\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta})}{p+1} \\\\ = &amp;\\frac{(\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta})^\\top(X^\\top X)(\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta})}{(p+1)\\hat{\\sigma}^2} \\\\ =&amp;\\frac{(\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta})^\\top\\left[{\\textrm{se}}(\\hat{\\boldsymbol{\\beta}})\\right]^{-1}(\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta})/(p+1)}{\\hat{\\sigma}^2/\\sigma^2}\\sim F_{p+1,n-p-1} \\end{aligned} \\] To see the last term follows \\(F_{p+1,n-p-1}\\), suppose \\(\\boldsymbol{z}\\sim\\mathcal{N}_{p+1}(\\boldsymbol{0},\\Sigma)\\), then we have \\[ \\boldsymbol{z}^\\top\\Sigma^{-1}\\boldsymbol{z}=\\boldsymbol{e}^\\top\\Sigma^{1/2}\\Sigma^{-1}\\Sigma^{1/2}\\boldsymbol{e}=\\boldsymbol{e}^\\top\\boldsymbol{e}\\sim\\chi_{p+1}^2. \\] We bound the statistic from above since \\(F\\)-distribution is one-sided over \\([0,\\infty)\\) and we want to measure how close \\(\\boldsymbol{\\beta}\\) is to \\(\\hat{\\boldsymbol{\\beta}}\\). The result is \\((\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta})^\\top X^\\top X(\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta})\\leq (p+1)\\hat{\\sigma}^2F_{p+1,n-p-1}^{(\\alpha)}\\). library(ellipse) ## ## Attaching package: &#39;ellipse&#39; ## The following object is masked from &#39;package:graphics&#39;: ## ## pairs data(savings) result &lt;- lm(sr ~ pop15 + pop75 + dpi + ddpi, savings) # estimate linear regression coefficients conf &lt;- confint(result) # compute confidence intervals for every predictor plot(ellipse(result, c(&#39;pop15&#39;, &#39;pop75&#39;)), type=&quot;l&quot;, xlim=c(-0.9,0)) # plot the confidence region points(result$coef[&#39;pop15&#39;], result$coef[&#39;pop75&#39;], pch=18) # add the estimate to the plot points(0, 0, pch=1) # add the origin to the plot abline(v=conf[&#39;pop15&#39;,], lty=2) # add the confidence interval for pop15 abline(h=conf[&#39;pop75&#39;,], lty=2) # add the confidence interval for pop17 Notice that simultaneous CI may contain points that are excluded by individual CI and vice versa. The t value and P(&gt;|t|) fields are respectively the \\(t\\)-statistic and the p-value of two-sided \\(t\\)-test on whether that single parameter is significant (see Section 2.6).↩︎ This result is the last row of R summary(lm(...)) output (see Section 2.6)↩︎ "],["prediction.html", "Chapter 4 Prediction 4.1 Prediction Interval 4.2 R Example", " Chapter 4 Prediction Two types of predictions: Prediction of a future observation: \\(y_0\\) Prediction of the future mean response: \\(\\mathbb{E}[Y|X=x_0]\\) 4.1 Prediction Interval We aim to use the predicted value to quantify the high-probability range of the actual response. Let \\((\\boldsymbol{x}_0, y_0)\\) be a future data pair and \\(\\hat{y}_0:=\\boldsymbol{x}_0^\\top\\hat{\\boldsymbol{\\beta}}\\) be its predicted value. 4.1.1 Future observation We already know that \\(y_0\\sim \\mathcal{N}_1(\\boldsymbol{x}_0^\\top\\hat{\\boldsymbol{\\beta}},\\sigma^2)\\) \\(\\hat{y}_0\\sim\\mathcal{N}_1(\\boldsymbol{x}_0^\\top\\hat{\\boldsymbol{\\beta}},\\sigma^2\\boldsymbol{x}_0^\\top(X^\\top X)^{-1}\\boldsymbol{x}_0)\\) \\(y_0\\) and \\(\\hat{y}_0\\) are independent. This implies \\(\\hat{y}_0-y_0\\sim\\mathcal{N}_1(0,\\sigma^2(1+\\boldsymbol{x}_0^\\top(X^\\top X)^{-1}\\boldsymbol{x}_0))\\) and therefore, \\[ \\frac{\\hat{y}_0-y_0}{\\hat{\\sigma}\\sqrt{1+\\boldsymbol{x}_0^\\top(X^\\top X)^{-1}\\boldsymbol{x}_0}} = \\frac{(\\hat{y}_0-y_0)/\\big(\\sigma\\sqrt{1+\\boldsymbol{x}_0^\\top(X^\\top X)^{-1}\\boldsymbol{x}_0}\\big)}{\\hat{\\sigma}/\\sigma}\\sim t_{n-p-1} \\] The prediction interval can then be constructed by \\[ \\boldsymbol{x}_0^\\top\\hat{\\boldsymbol{\\beta}}\\pm t_{n-p-1}^{(\\alpha/2)}\\hat{\\sigma}\\sqrt{1+\\boldsymbol{x}_0^\\top(X^\\top X)^{-1}\\boldsymbol{x}_0} \\] 4.1.2 Future mean response This can be derived directly from the distribution of \\(\\hat{y}_0\\): \\[ \\boldsymbol{x}_0^\\top\\hat{\\boldsymbol{\\beta}}\\pm t_{n-p-1}^{(\\alpha/2)}\\hat{\\sigma}\\sqrt{\\boldsymbol{x}_0^\\top(X^\\top X)^{-1}\\boldsymbol{x}_0} \\] This is exactly the confidence interval for the (BLUE) estimator \\(\\hat{\\psi}=\\boldsymbol{x}_0^\\top\\hat{\\boldsymbol{\\beta}}\\) of the quantity \\(\\psi=\\boldsymbol{x}_0^\\top\\boldsymbol{\\beta}\\). 4.2 R Example data(savings) result &lt;- lm(sr ~ pop15 + pop75 + dpi + ddpi, savings) x0 &lt;- data.frame(pop15=35, pop75=2, dpi=1000, ddpi=4) predict(result, x0, interval=&quot;confidence&quot;) # prediction interval for E[Y|X=x0] ## fit lwr upr ## 1 10.34321 9.093452 11.59297 predict(result, x0, interval=&quot;prediction&quot;) # prediction interval for y0 ## fit lwr upr ## 1 10.34321 2.582946 18.10347 "],["diagnostics.html", "Chapter 5 Diagnostics 5.1 Checking Error Assumptions 5.2 Finding Unusual Points 5.3 Checking Model Structure", " Chapter 5 Diagnostics 5.1 Checking Error Assumptions 5.1.1 Constant Noise Variance Residual plot plots residuals \\(\\hat{\\varepsilon}_i\\) against predictions \\(\\hat{y}_i\\). It can be used to check Linear mean: linear model (i.e., \\(\\mathbb{E}[Y|X]=X\\boldsymbol{\\beta}\\)) implies \\(\\textrm{Cov}(\\hat{\\boldsymbol{\\varepsilon},\\hat{\\boldsymbol{y}}})=\\widehat{\\textrm{Cov}}(\\hat{\\boldsymbol{\\varepsilon}},\\hat{\\boldsymbol{y}})=0\\). So, the plot should display no pattern but appear as a evenly spread horizontal band of points with mean zero. For population covariance, notice \\[ \\textrm{Cov}(\\hat{\\boldsymbol{\\varepsilon}},\\hat{\\boldsymbol{y}}) = \\textrm{Cov}((I-H)\\boldsymbol{y},H\\boldsymbol{y}) = (I-H)\\textrm{Var}(\\boldsymbol{y})H^\\top=\\sigma^2(I-H)H^\\top = 0 \\] where \\(H\\) is the hat matrix \\(X(X^\\top X)^{-1}X^\\top\\). This derivation used the homoscedasticity and independence of the noise distribution. For sample covariance, recall \\(\\sum_{i\\in[n]}\\hat{\\varepsilon}_i=0\\) as \\(\\hat{\\boldsymbol{\\varepsilon}}\\in\\mathcal{C}(X)^\\perp\\) and \\(\\mathbb{1}\\in\\mathcal{C}(X)\\). Then, we have \\[ \\sum_{i\\in[n]}(\\hat{\\varepsilon}_i-\\bar{\\hat{\\varepsilon}})(\\hat{y}_i-\\bar{\\hat{y}}) = \\sum_{i\\in[n]}\\hat{\\varepsilon}_i(\\hat{y}_i-\\bar{\\hat{y}}) = \\sum_{i\\in[n]}\\hat{\\varepsilon}_i\\hat{y}_i=\\hat{\\boldsymbol{\\varepsilon}}^\\top\\hat{\\boldsymbol{y}} = \\boldsymbol{y}^\\top(I-H)H\\boldsymbol{y} = 0. \\] This derivation is independent of the noise distribution as all elements are determined by the algorithm. Homoscedasticity and heteroscedasticity: whether the dispersion of \\(\\hat{\\varepsilon}_i\\) is constant (resp. increases) in \\(\\hat{y}_i\\) for homoscedasticity (resp. heteroscedasticity). library(faraway) data(savings) result &lt;- lm(sr ~ pop15 + pop75 + dpi + ddpi, savings) plot(result$fitted, result$residual, xlab=&quot;Fitted&quot;, ylab=&quot;Residuals&quot;) # ehat v.s. yhat abline(h=0) plot(result$fitted, abs(result$residual), xlab=&quot;Fitted&quot;, ylab=&quot;|Residuals|&quot;) # |ehat| v.s. yhat summary(lm(abs(result$residual) ~ result$fitted)) ## ## Call: ## lm(formula = abs(result$residual) ~ result$fitted) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.8395 -1.6078 -0.3493 0.6625 6.7036 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.8398 1.1865 4.079 0.00017 *** ## result$fitted -0.2035 0.1185 -1.717 0.09250 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.163 on 48 degrees of freedom ## Multiple R-squared: 0.05784, Adjusted R-squared: 0.03821 ## F-statistic: 2.947 on 1 and 48 DF, p-value: 0.0925 The last method regress \\(|\\hat{\\varepsilon}_i|\\)3 onto \\(\\hat{y}_i\\) and test \\(H_0:\\beta_1=0\\) v.s. \\(H_A:\\beta_1\\neq 0\\). 5.1.2 Checking Noise Normality 5.1.2.1 QQ-plot Sort the residuals \\(\\hat{\\varepsilon}_{[1]}\\leq \\hat{\\varepsilon}_{[2]}\\leq \\dots\\leq \\hat{\\varepsilon}_{[n]}\\) — order statistic Compute the \\(n\\) percentiles \\(u_i = F^{-1}\\left(\\frac{i}{n+1}\\right)\\) where \\(F\\) is the CDF of the testing distribution Plot \\(\\hat{\\varepsilon}_{[i]}\\) against \\(u_i\\) The data follows the distribution if the scatter points lie on a straight line ## Example QQ plot qqnorm(result$residual, ylab=&quot;Residuals&quot;) qqline(result$residual) Remarks Should be tested after ensuring linear mean and constant noise variance Possibilities of non-normality (both 1 and 2 are heavy-tailed distribution): Skewed distribution: log-normal Long-tailed distribution: Cauchy Short-tailed distribution: uniform with finite support 5.1.3 Shapiro-Wilk test for normality \\(H_0\\): \\(x_1,\\dots,x_n\\) are sampled from a normally distributed population shapiro.test(result$residual) ## ## Shapiro-Wilk normality test ## ## data: result$residual ## W = 0.98698, p-value = 0.8524 Shapiro-Wilk test is not very helpful compared to QQ plot because When \\(n\\) is small, the test has little power When \\(n\\) is large, we can use the asymptotic distribution (e.g., central limit theorem) to do inference (i.e., hypothesis tests or CI).4 5.2 Finding Unusual Points Outliers: large difference between the response \\(y_i\\) and the mean \\(\\boldsymbol{x}_i^\\top\\boldsymbol{\\beta}\\) High-leverage points: large difference between the predictor vector \\(\\boldsymbol{x}_i\\) for the \\(i\\)th case and the center of the \\(X\\)-data 5.2.1 Leverage The leverage of a point \\(i\\) is \\(h_i:=H_{ii}\\) where \\(H\\) is the hat matrix \\(H:=X(X^\\top X)^{-1}X^\\top\\). \\(h_i\\) depends on on \\(X\\) \\(\\textrm{Var}(\\hat{\\varepsilon}_i)=\\sigma^2(1-h_i)\\) \\(\\sum_{i\\in[n]}h_i=\\textrm{Tr}(H)=p+1\\) \\(\\frac{1}{n}\\leq h_i \\leq 1\\) for all \\(i\\in[n]\\) Remarks Average leverage is \\(\\frac{p+1}{n}\\), so generally leverage larger than \\(\\frac{2(p+1)}{n}\\) can be considered high Let \\(\\tilde{\\boldsymbol{x}}\\) be the “reduced” data by removing the first constant value. Then, \\[ h_i=\\frac{1}{n}+(\\tilde{\\boldsymbol{x}}_i-\\bar{\\tilde{\\boldsymbol{x}}})^\\top(\\tilde{X}_C^\\top\\tilde{X}_C)^{-1}(\\tilde{\\boldsymbol{x}}_i-\\bar{\\tilde{\\boldsymbol{x}}}) \\] where \\(\\tilde{X}_C=\\begin{bmatrix}(\\tilde{\\boldsymbol{x}}_1-\\bar{\\boldsymbol{x}}) &amp; \\dots &amp; (\\tilde{\\boldsymbol{x}}_n-\\bar{\\boldsymbol{x}})\\end{bmatrix}^\\top\\) 5.2.2 Half-normal Plot Sort \\(h_{[1]}\\leq h_{[2]}\\leq \\dots \\leq h_{[n]}\\) Compute \\(u_i=F^{-1}\\left(\\frac{i}{n+1}\\right)\\) for \\(i\\in[n]\\) Plot \\(h_{[i]}\\) v.s. \\(u_i\\) A high-leverage point usually diverges from the rest of the points. ## Example for half normal test, high-leverage points usually have large-deviating predictor values data(savings) result &lt;- lm(sr ~ ., data=savings) halfnorm(lm.influence(result)$hat, nlab=2, ylab=&quot;Leverage&quot;) savings[c(44,49),] ## sr pop15 pop75 dpi ddpi ## United States 7.56 29.81 3.43 4001.89 2.45 ## Libya 8.89 43.69 2.07 123.58 16.71 5.2.3 Studentized Residuals Studentized residuals rescales the residual for unit variance using the leverage of that data point: \\[ r_i=\\frac{\\hat{\\varepsilon}_i}{\\hat{\\sigma}\\sqrt{1-h_i}} \\] Theorem 5.1 \\(\\frac{r_i^2}{n-p-1}\\sim\\textrm{Beta}(\\frac{1}{2},\\frac{n-p-2}{2})\\). Proof. \\[ \\frac{r_i^2}{n-p-1} = \\frac{\\hat{\\varepsilon}_i^2/(\\sigma^2(1-h_i))}{\\hat{\\sigma}^2/\\sigma^2}. \\] It directly follows that \\(\\hat{\\varepsilon}_i^2/(\\sigma^2(1-h_i))\\sim\\chi_1^2\\) and \\(\\hat{\\sigma}^2/\\sigma^2\\sim\\chi_{n-p-1}^2\\). However, it’s not \\(F\\)-distribution because they are not independent. Let \\(\\boldsymbol{\\varepsilon}\\sim\\mathcal{N}_n(\\boldsymbol{0},I)\\) the above expression is equivalent to \\(\\frac{r_i^2}{n-p-1}\\sim\\frac{\\boldsymbol{\\varepsilon}^\\top R^\\top\\boldsymbol{e}_i\\boldsymbol{e}_i^\\top R\\boldsymbol{\\varepsilon}/(1-h_i)}{\\boldsymbol{\\varepsilon}^\\top R\\boldsymbol{\\varepsilon}}=:\\frac{\\boldsymbol{\\varepsilon}^\\top Q\\boldsymbol{\\varepsilon}}{\\boldsymbol{\\varepsilon}^\\top R\\boldsymbol{\\varepsilon}}\\) where \\(\\boldsymbol{e}_i\\) is the \\(i\\)-th unit vector. First notice that \\(Q\\) is symmetric and \\[ Q^2 = R^\\top\\boldsymbol{e}_i\\boldsymbol{e}_i^\\top RR\\boldsymbol{e}_i\\boldsymbol{e}_i^\\top R/(1-h_i)^2=R\\boldsymbol{e}_i\\boldsymbol{e}_i^\\top R/(1-h_i)=Q. \\] Thus, \\(Q\\) is a projection matrix. Notice that \\(\\boldsymbol{\\varepsilon}^\\top R\\boldsymbol{\\varepsilon}-\\boldsymbol{\\varepsilon}^\\top Q\\boldsymbol{\\varepsilon}=\\boldsymbol{\\varepsilon}^\\top(R-Q)\\boldsymbol{\\varepsilon}\\) since \\(RQ=QR=Q\\). The last step is to show that \\(\\boldsymbol{\\varepsilon}^\\top(R-Q)\\boldsymbol{\\varepsilon}\\) is independent of \\(\\boldsymbol{\\varepsilon}^\\top Q\\boldsymbol{\\varepsilon}\\). This is straightforward because \\((R-Q)\\boldsymbol{\\varepsilon}\\) is independent of \\(Q\\boldsymbol{\\varepsilon}\\) and the numerator and denominator are functions of each of them. Notice \\(r_i\\) does not follow \\(t\\)-distribution because the numerator and denominator are not independent. A general form of \\(t\\)-distribution requires the numerator and denominator lying on orthogonal subspaces. Studentized residuals are generally preferred than raw data in diagnostic plots (e.g., QQ-plot or residual plots). 5.2.4 Externally Studentized Residuals Exclude point \\(i\\) and estimate \\(\\hat{\\boldsymbol{\\beta}}_{(i)}\\) using the rest \\(n-1\\) points. Compute \\(\\hat{y}_{(i)}=\\boldsymbol{x}_i\\hat{\\boldsymbol{\\beta}}_{(i)}\\). The externally studentized residual define the residual as the prediction error: \\[ \\begin{aligned} t_i &amp;= \\frac{y_i-\\hat{y}_{(i)}}{\\hat{\\sigma}_{(i)}\\sqrt{1+\\boldsymbol{x}_i^\\top (X_{(i)}^\\top X_{(i)})^{-1}\\boldsymbol{x}_i}} \\sim t_{(n-1)-(p+1)} \\end{aligned} \\] Theorem 5.2 The externally studentized residual has two equivalent forms \\[ t_i=\\frac{\\hat{\\varepsilon}_i}{\\hat{\\sigma}_{(i)}\\sqrt{1-h_i}}=r_i\\left(\\frac{n-(p+1)-1}{n-(p+1)-r_i^2}\\right)^{1/2} \\] The first term is a contrast to the (internally) studentized residual The second line offers easier computation method: only need to do one regression with all \\(n\\) points A reasonable of threshold for influential point is \\(1\\). 5.2.5 Multiple Hypothesis Tests Since we need to repeat the test \\(n\\) times (on \\(H_0\\) — observation \\(i\\) is an outlier), we need to apply some adjustment to the significance level to avoid excess rejection on the set level. 5.2.5.1 Bonferroni Correction \\[ \\begin{aligned} \\textrm{Type I Error} &amp;= \\mathbb{P}_{H_0}(\\textrm{reject at least one test}) \\\\ &amp;\\leq \\sum_{i\\in[n]}\\mathbb{P}_{H_0}(\\textrm{reject test }i) \\\\ &amp;=n\\alpha^{\\textrm{adj}} \\approx \\alpha \\end{aligned} \\] So, instead we test each observation with significance level \\(\\alpha/n\\). 5.2.6 Influential Points An influential point is one whose removal from the d4ataset would cause a large change in the fit. Outliers or high-leverage points can both be influential points. Definition 5.1 (Cook's Distance) Cook statistic is defined as \\[ \\begin{aligned} D_i&amp;=\\frac{(\\hat{y}_i-\\hat{y}_{(i)})^\\top(\\hat{y}_i-\\hat{y}_{(i)})}{\\hat{\\sigma}(p+1)} \\\\ &amp;= \\frac{1}{p+1}r_i^2\\frac{h_i}{1-h_i} \\end{aligned} \\] Cook’s statistic measures a combination of residual effect and leverage effect In practice, usually 1 is used as a critical value for potential influential point data(savings) ## Compute Cook&#39;s distance cook &lt;- cooks.distance(result) halfnorm(cook, nlab=3, ylab=&quot;Cook&#39;s distance&quot;) ## Fit the model without the influential point result.libya &lt;- lm(sr ~ pop15 + pop75 + dpi + ddpi, data=savings, subset=(cook &lt; max(cook))) summary(result.libya) ## ## Call: ## lm(formula = sr ~ pop15 + pop75 + dpi + ddpi, data = savings, ## subset = (cook &lt; max(cook))) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.0699 -2.5408 -0.1584 2.0934 9.3732 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 24.5240460 8.2240263 2.982 0.00465 ** ## pop15 -0.3914401 0.1579095 -2.479 0.01708 * ## pop75 -1.2808669 1.1451821 -1.118 0.26943 ## dpi -0.0003189 0.0009293 -0.343 0.73312 ## ddpi 0.6102790 0.2687784 2.271 0.02812 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.795 on 44 degrees of freedom ## Multiple R-squared: 0.3554, Adjusted R-squared: 0.2968 ## F-statistic: 6.065 on 4 and 44 DF, p-value: 0.0005617 ## Compare this with the full summary pannel where the ddpi coefficient changes about 50% ## and the R2 value increases by ~0.02 ## We don&#39;t like our model to be so sensitive to just one country summary(result) ## ## Call: ## lm(formula = sr ~ ., data = savings) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.2422 -2.6857 -0.2488 2.4280 9.7509 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 28.5660865 7.3545161 3.884 0.000334 *** ## pop15 -0.4611931 0.1446422 -3.189 0.002603 ** ## pop75 -1.6914977 1.0835989 -1.561 0.125530 ## dpi -0.0003369 0.0009311 -0.362 0.719173 ## ddpi 0.4096949 0.1961971 2.088 0.042471 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.803 on 45 degrees of freedom ## Multiple R-squared: 0.3385, Adjusted R-squared: 0.2797 ## F-statistic: 5.756 on 4 and 45 DF, p-value: 0.0007904 ## Compute changes in coefficients ## The lower left points are likely influential points ## Add identify(result.inf$coef[, 2], result.inf$coef[, 3]) to enable interactive tools to identify ## those points by clicking result.inf &lt;- lm.influence(result) plot(result.inf$coef[,2], result.inf$coef[,3], xlab=&quot;Change in beta2&quot;, ylab=&quot;Change in beta3&quot;) ## Print points rownames(savings)[c(23, 46, 49)] ## [1] &quot;Japan&quot; &quot;Zambia&quot; &quot;Libya&quot; ## Fit the model w/o Japan result.japan &lt;- lm(sr ~ pop15 + pop75 + dpi + ddpi, data=savings, subset=(rownames(savings)!=&quot;Japan&quot;)) summary(result.japan) ## ## Call: ## lm(formula = sr ~ pop15 + pop75 + dpi + ddpi, data = savings, ## subset = (rownames(savings) != &quot;Japan&quot;)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.997 -2.592 -0.115 2.032 10.157 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 23.9401714 7.7839968 3.076 0.00361 ** ## pop15 -0.3679015 0.1536296 -2.395 0.02096 * ## pop75 -0.9736743 1.1554502 -0.843 0.40397 ## dpi -0.0004706 0.0009191 -0.512 0.61116 ## ddpi 0.3347486 0.1984457 1.687 0.09871 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.738 on 44 degrees of freedom ## Multiple R-squared: 0.277, Adjusted R-squared: 0.2113 ## F-statistic: 4.214 on 4 and 44 DF, p-value: 0.005649 5.3 Checking Model Structure Tests that imply the underlying structure of the model as well as suggestions on how to improve the structure of the model. 5.3.1 Exploratory analysis Plot \\(y\\) v.s. each individual \\(x_i\\) to investigate the relationship and/or linearity between each individual predictor. Usually done before fitting a model Drawback: other predictors may affect the relationship between \\(y\\) and \\(x_i\\) 5.3.2 Partial regression plot Isolate the effect of \\(x_i\\) on \\(y\\) Regress \\(y\\) on all \\(x\\) except \\(x_i\\), get residuals \\(\\hat{\\delta}\\) — take out effect of other \\(X\\) from \\(y\\) Regress \\(x_i\\) on all \\(x\\) except \\(x_i\\), get residuals \\(\\hat{\\gamma}\\) — take out effect of other \\(X\\) from \\(x_i\\) Plot \\(\\hat{\\delta}\\) v.s. \\(\\hat{\\gamma}\\) The slope is \\(\\hat{\\beta}_j\\) Can be used for linearity, outliers, and influential point tests data(savings) ## Partial regression plot result &lt;- lm(sr ~ pop15 + pop75 + dpi + ddpi, data=savings) delta &lt;- residuals(lm(sr ~ pop75 + dpi + ddpi, data=savings)) gamma &lt;- residuals(lm(pop15 ~ pop75 + dpi + ddpi, data=savings)) plot(gamma,delta, xlab=&quot;Pop15 Residuals&quot;, ylab=&quot;Saving Residuals&quot;) temp &lt;- lm(delta ~ gamma) abline(reg=temp) ## The slope of the partial regression plot ## Notice this is the same as beta(pop15) in the summary after this coef(temp) ## (Intercept) gamma ## -1.545720e-16 -4.611931e-01 coef(result) ## (Intercept) pop15 pop75 dpi ddpi ## 28.5660865407 -0.4611931471 -1.6914976767 -0.0003369019 0.4096949279 Must have the absolute value as we are testing for variance.↩︎ Notice that we only requires normality in the inference step.↩︎ "],["problem-with-predictors.html", "Chapter 6 Problem with Predictors 6.1 Errors in predictors 6.2 Change of scale 6.3 Standardize Variables 6.4 Colinearity", " Chapter 6 Problem with Predictors 6.1 Errors in predictors Consider simple regression with observation and measurement errors. \\[\\begin{align} &amp;Y^o = Y + \\varepsilon \\\\ &amp;X^o = X + \\delta \\\\ &amp;Y = \\beta_0 + \\beta_1 X \\end{align}\\] So, the relationshiop of the observations is \\[ Y^o = \\beta_0 + \\beta_1 X^o + \\varepsilon - \\beta_1\\delta \\] The estimator is biased \\[ \\mathbb{E}{\\hat{\\beta}_1} =\\beta_1 \\frac{\\sigma_x^2 + \\sigma_{x\\delta}}{\\sigma_x^2 + \\sigma_{\\delta}^2 + \\sigma_{x\\delta}} \\] When \\(X\\) and \\(\\delta\\) are uncorrelated: - \\(\\hat{\\beta}_1\\) biased towards zero - negligible when \\(\\sigma_x&gt;&gt;\\sigma_\\delta\\) 6.2 Change of scale \\[ x_j \\mapsto \\frac{x_j-a}{b}\\;\\textrm{ or } \\; y\\mapsto \\frac{y-a}{b} \\] Benefit of changing scales: predictors with similar magnitude are easier to compare improve numerical stability aid interpretation Consequences: Rescaling \\(x_i\\) does not change the projection matrix \\[ \\mathbb{E}[Y|\\boldsymbol{x}]= \\beta_0 + \\dots + \\frac{x_j}{b}\\cdot (b\\beta_j) - \\frac{a}{b}(b\\beta_j) + \\dots \\] leaving \\(t\\)/\\(F\\) tests, \\(R^2\\), and \\(\\hat{\\sigma}^2\\) unchanges, but \\(\\hat{\\beta}_j\\mapsto b\\hat{\\beta}_j\\) and \\(\\hat{\\beta}_0\\mapsto \\hat{\\beta}_0 + a\\hat{\\beta}_j\\) Rescaling \\(y\\) makes \\(\\hat{\\beta}_i\\mapsto\\hat{\\beta}_i/b\\), \\(\\forall i\\in[n]\\), and \\(\\hat{\\beta}_0\\mapsto(\\hat{\\beta}_0-a)/b\\) \\[ \\mathbb{E}[Y_{\\textrm{new}}|\\boldsymbol{x}] = \\frac{\\beta_0-a}{b} + \\sum_{i\\in[n]}\\frac{x_i\\beta_i}{b}. \\] \\(\\hat{Y}\\) and \\(\\bar{Y}\\) will be scaled similarly, leaving the \\(t\\)/\\(F\\) tests and \\(R^2\\) unchanges but \\(\\hat{\\sigma}\\mapsto\\hat{\\sigma}/b\\). 6.3 Standardize Variables Can compare coefficients directly Help numerical stability Harder to interpret 6.4 Colinearity Consequence: Imprecise estimate of \\(\\beta\\) due to large standard error Detection Pairwise correlation: correlation matrix General (non-pairwise) linearity: regress \\(x_j\\) onto other predictors and look for large \\(R^2\\) (denoted as \\(R_i^2\\)) — \\(\\frac{1}{1-R^2_i}\\) is called the variance inflation factor Condition number of \\(X^\\top X\\): \\(\\kappa =\\sqrt{\\lambda_{(1)}/\\lambda_{(p+1)}}\\) "],["problem-with-error.html", "Chapter 7 Problem with Error 7.1 ", " Chapter 7 Problem with Error 7.1 "]]
