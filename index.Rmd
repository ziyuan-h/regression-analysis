--- 
title: "Regression Analysis Notes"
author: "Ziyuan Huang"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: rstudio/bookdown-demo
description: "A study notes for regression analysis."
---


# Prerequisites

## Regression Model

Given *response* $Y\in\mathbb{R}$ and *predictors* $\underline{X}=(X_1,\dots,X_p)\in\mathbb{R}^p$, the goal is to model the relationship between $Y$ and $\underline{X}$ (called to *regress $Y$ onto $\underline{X}$*) by
$$
  Y=f(\underline{X})+\varepsilon
$$
where $f(\cdot)$ is unknown and $\varepsilon$ is the noise. 

The **input** is a dataset of size $n$: $\left\{(x_{ij})_{j\in[p]},y_i\right\}_{i\in[n]}$.


```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```


#### Linear Regression Model {-}

Assume $f(X) = \beta_0+\beta_1X_1+\dots+\beta_pX_p$.

- Reduce estimation of functions to estimation of parameters.
- A linear model is linear in *parameters* not linear in predictors!
  - Predictors can be transformed features such as $\ln(X_1)$ or $X_1X_2$
  - Generalized linear model:
  $$g(\mathbb{E}[Y|X])=\beta_0+\sum_{i\in[p]}\beta_iX_i$$
  for some *known* function $g$.


## Continuous Random Variables

::: {.definition name="t-distribution"}
  The *$t$-distribution* with *degrees of freedom* $n$ is determined by the following pdf
  $$
    t_n \sim f(x) = \frac{\Gamma(\frac{n+1}{2})}{\sqrt{n\pi}\cdot\Gamma(\frac{n}{2})}\left(1+\frac{x^2}{n}\right)^{-\frac{n+1}{2}}
  $$
  where $\Gamma(z) = \int_0^\infty t^{z-1} e^{-t}dt$.
:::

  - $t$-distribution is symmetric around zero, bell-shaped, but has heavier tails than normal
  - $t_n\to \mathcal{N}(0,1)$ as $n\to\infty$
  
--- 

::: {.definition name="Beta distribution"}
  *Beta distribution* with parameters $a>0$ and $b>0$ has pdf
  $$
    \textrm{Beta}(a, b)\sim f(x)=\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}x^{a-1}(1-x)^{b-1},\quad 0<x<1 .
  $$
  The inverse coefficient is also called the beta-function $B(a,b)=\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$
:::

- Beta distribution is a generalization of uniform distribution to distributions over $[0,1]$
- $\textrm{Beta}(1,1)=\textrm{Unif}(0,1)$
- *U-shaped* when $a<1$ and $b<1$; and is *bell-shaped* when $a>1$ and $b>1$
- Skewed towards 0 when $a<b$; and is skewed towards 1 when $a>b$.

::: {.theorem name="Beta Conjugate"}
Beta distribution is the conjugate distribution for binomial distribution.

- If prior is $p\sim\textrm{Unif}(0,1)$, let $X\sim\textrm{Bin}(n,p)$, then
  $$
    \mathbb{P}(X=k) = \int\begin{pmatrix}n \\ k\end{pmatrix}x^k(1-x)^{n-k}dx = \frac{1}{n+1}
  $$
  meaning the expected number of successes are uniformly distributed.
- Let $p\sim\textrm{Beta}(a,b)$ be the prior and the number of successes be $k$. Then, the posterior is $\textrm{Beta}(a+k,b+n-k)$. Thus, $a-1$ represents the number of prior successes and $b-1$ represents the number of prior failures.
:::


::: {.theorem name="Beta Order Statistic"}
  Let $X$ be the distribution of the $k$-th smallest element of $n$ uniform and independent tosses Then, $X\sim\textrm{Beta}(k,n-k)$.
:::
::: {.proof}
  Observe $\mathbb{P}(X\leq t) = \mathbb{P}(\textrm{at least k tosses land in [0,t]})=\sum_{i\geq k}\begin{pmatrix}n\\i\end{pmatrix}t^i(1-t)^{n-i}$. Thus,
  $$
    \begin{aligned}
    f_{X}(t) &= \sum_{i\geq k}\begin{pmatrix}n\\i\end{pmatrix}\left(it^{i-1}(1-t)^{n-i} - (n-i)t^i(1-t)^{n-i-1}\right) \\
    &= \begin{pmatrix}n \\ k\end{pmatrix}kt^{k-1}(1-t)^{n-k} + \sum_{i=k}^{n-1}\left[-(n-i)\begin{pmatrix}n \\ i\end{pmatrix}+(i+1)\begin{pmatrix}n \\ i+1\end{pmatrix}\right] t^i(1-t)^{n-i-1} \\
    & = \frac{n!}{(k-1)!(n-k)!}t^{k-1}(1-t)^{n-k} \sim \textrm{Beta}(k, n-k+1)
    \end{aligned}
  $$
  Notice that for integer $n$, $\Gamma(n)=(n-1)!$.
:::

---
  
::: {.definition name="Gamma distribution"}
  *Gamma distribution* with *shape* parameter $\alpha$ and *scale* parameter $\theta$ has pdf
  $$
    \textrm{Gamma}(\alpha,\theta) \sim f(x;\alpha,\theta) = \frac{1}{\Gamma(\alpha)\theta^\alpha}x^{\alpha - 1} e^{-\frac{x}{\theta}},\quad 0\leq x<\infty.
  $$
:::

::: {.lemma name="Infinite divisibility"}
  Let $X_i\sim\textrm{Gamma}(\alpha_i,\theta)$ for $i\in[n]$ be *independent*, then
  $$
    \sum_{i=1}^nX_i \sim \textrm{Gamma}\left(\sum_{i\in[n]}\alpha_i,\theta\right).
  $$
  This implies that Gamma distribution is additive in the shape parameter and that it can be decomposed as the sum of arbitrary independent random variables --- a property called **infinite divisibility**.
:::

::: {.proof}
  It is sufficient to show the additivity property for $X_1$ and $X_2$.
$$
  \begin{aligned}
    f_{X_1+X_2}(t) &= \int_{0}^\infty f_{X_1}(x)f_{X_2}(t-x)dx \\
    &= \int_{0}^t\frac{x^{\alpha_1-1}e^{-x/\theta}}{\Gamma(\alpha_1)\theta^{\alpha_1}}\frac{(t-x)^{\alpha_2-1}e^{-(t-x)/\theta}}{\Gamma(\alpha_1)\theta^{\alpha_2}}dx \\
    &=e^{-t/\theta}\int_0^t \frac{x^{\alpha_1-1}(t-x)^{\alpha_2-1}}{\Gamma(\alpha_1)\Gamma(\alpha_2)\theta^{\alpha_1+\alpha_2}}dx \\
    &\overset{x:=tz}{=}\frac{t^{\alpha_1+\alpha_2-1}e^{-t/\theta}}{\Gamma(\alpha_1+\alpha_2)\theta^{\alpha_1+\alpha_2}}\int_0^1\frac{\Gamma(\alpha_1+\alpha_2)}{\Gamma(\alpha_1)\Gamma(\alpha_2)}z^{\alpha_1-1}(1-z)^{\alpha_2-1}dz \\
    &=\frac{t^{\alpha_1+\alpha_2-1}e^{-t/\theta}}{\Gamma(\alpha_1+\alpha_2)\theta^{\alpha_1+\alpha_2}} \sim \textrm{Gamma}(\alpha_1+\alpha_2,\theta)
  \end{aligned}
$$
where the last equality used the fact that $\textrm{Beta}(\alpha_1,\alpha_2)\sim \frac{\Gamma(\alpha_1+\alpha_2)}{\Gamma(\alpha_1)\Gamma(\alpha_2)}z^{\alpha_1-1}(1-z)^{\alpha_2-1}$.
:::


*Remarks*

  - $\textrm{Gamma}(1,\theta)=\textrm{Exp}(1/\theta)$ and $\textrm{Gamma}(n,\theta)=\sum_{i=1}^n\textrm{Exp}(1/\theta)$: $\textrm{Gamma}(n,\theta)$ models the waiting time for *the next $n$* independent events to occur, compared to the exponential distribution that only models the very *next* event.
  - Scaling property: $X\sim\textrm{Gamma}(\alpha,\theta)\implies cX\sim\textrm{Gamma}(\alpha,c\theta)$

::: {.theorem name="Beta and Gamma"}
 Let $X\sim\textrm{Gamma}(\alpha,\theta)$ and $Y\sim\textrm{Gamma}(\beta,\theta)$ be independent. Then, $\frac{X}{X+Y}\sim\textrm{Beta}(\alpha,\beta)$ and $\frac{X}{X+Y}$ is independent of $X+Y$.
:::
:::{.proof}
We apply the transformation rule: $f_Y(y)=f_X(x)\left|\frac{dx}{dy}\right|$. In this case, $x=(X,Y)$ and $y=(\frac{X}{X+Y}, X+Y)$. The mapping $y\mapsto x$ is $x_1=y_1y_2$ and $x_2=(1-y_1)y_2$.
$$
  \left|\frac{dx}{dy}\right| = \left|\begin{pmatrix}
    y_2 & y_1 \\ -y_2 & 1-y_1
  \end{pmatrix}\right| = 
  \left|\begin{pmatrix}
    X+Y & \frac{X}{X+Y} \\ -X-Y & \frac{Y}{X+Y}
  \end{pmatrix}\right| = Y + X.
$$
Thus, the joint distribution of $(\frac{X}{X+Y}, X+Y)$ equals
$$
  \begin{aligned}
    f_{\frac{X}{X+Y}, X+Y}(w,t) &= f_X(wt)f_Y((1-w)t)t \\
    &\propto t(wt)^{\alpha-1}e^{-wt/\theta}((1-w)t)^{\beta-1}e^{-(1-w)t/\theta} \\
    &= \underbrace{w^{\alpha-1}(1-w)^{\beta-1}}_{\sim\textrm{Beta}(\alpha,\beta)}\cdot \underbrace{t^{\alpha+\beta-1}e^{-t/\theta}}_{\sim\textrm{Gamma}(\alpha+\beta,\theta)}
  \end{aligned}
$$
The separability of the two pdfs shows the independence.
:::

---


::: {.definition name="Chi-squared distribution"}
  A *chi-square distribution* with *degrees of freedom $n$* is the distribution of a sum of $n$ independent standard normal random variables, denoted as $\chi_n^2$.
:::

  - $\chi_n^2$ distribution is a special case of the gamma distribution $\chi_n^2=\textrm{Gamma}\left(\frac{n}{2},2\right)$
  
::: {.theorem name="Normal over Chi is t-distribution"}
Let $Z\sim\mathcal{N}(0,1)$ and $W\sim\chi_n^2$. Then, $\frac{Z}{\sqrt{W/n}}\sim t_n$.
:::
::: {.proof}
  We first calculate the distribution of $\sqrt{W/n}$, using the scaling property of the gamma distribution:
  $$
    \begin{aligned}
      \mathbb{P}(\sqrt{W/n}\leq t) &= \mathbb{P}(W/n\leq t^2) = \mathbb{P}\left(\textrm{Gamma}\left(\frac{n}{2},\frac{2}{n}\right)\leq t^2\right)
    \end{aligned}
  $$
Using the chain rule, we have 
$$
  f_{\sqrt{W/n}}(x) = 2t\left(\frac{n}{2}\right)^{\frac{n}{2}}\cdot\frac{x^{n-2}e^{-\frac{nx^2}{2}}}{\Gamma(\frac{n}{2})} = \frac{n^{n/2}}{2^{n/2-1}}\frac{x^{n-1}e^{-\frac{nx^2}{2}}}{\Gamma(\frac{n}{2})}
$$
Define $T:=Z/\sqrt{W/n}$. Then, use the [ratio distribution formula](https://en.wikipedia.org/wiki/Ratio_distribution) for independent random variables:
$$
  \begin{aligned}
    f_T(t) &= \int_{-\infty}^\infty|x|f_Z(tx)f_{\sqrt{W/n}}(x)dx = \frac{1}{\sqrt{2\pi}}\frac{n^{n/2}}{2^{n/2-1}\Gamma(\frac{n}{2})}\int_{0}^\infty|x| x^{n-1}\exp\left(-\frac{t^2x^2}{2}-\frac{n}{2}x^2\right)dx \\
    &=\frac{1}{\sqrt{2\pi}}\frac{n^{n/2}}{2^{n/2-1}\Gamma(\frac{n}{2})}\int_{0}^\infty x^{n}\exp\left(-\frac{t^2+n}{2}x^2\right)dx \\
    &\overset{z:=x^2}{=}\frac{1}{\sqrt{2\pi}}\frac{n^{n/2}}{2^{n/2-1}\Gamma(\frac{n}{2})}\frac{1}{2}\int_{0}^\infty z^{\frac{n-1}{2}}\exp\left(-\frac{t^2+n}{2}z\right)dz \\
    &=\frac{1}{\sqrt{2\pi}}\frac{n^{n/2}}{2^{n/2}\Gamma(\frac{n}{2})}\Gamma\left(\frac{n+1}{2}\right)\left(\frac{t^2+n}{2}\right)^{-\frac{n+1}{2}}\int_{0}^\infty f_{\textrm{Gamma}\left(\frac{n+1}{2},\frac{2}{t^2+n}\right)}(z)dz \\
    &= \frac{\Gamma(\frac{n+1}{2})}{\sqrt{\pi n}\Gamma(\frac{n}{2})}(1+t^2/n)^{-\frac{n+1}{2}}
  \end{aligned}
$$
This is exactly the pdf of $t_n$.
:::


---


::: {.definition name="F-distribution"}
  Let $U\sim\chi_m^2$, $v\sim\chi_n^2$, and $U,V$ be independent. Then, $\frac{U/m}{V/n}\sim F_{m,n}$, an $F$-distribution with $m$ and $n$ degrees of freedom.
:::


## Common Test Statistics

### T-test

Three types of t-test:

- One-sample t-test: comparing the mean of a population and a constant
  $$
    T = \frac{\bar{X}-\mu}{\hat{\sigma}/\sqrt{n}}
  $$

- Independent t-test: comparing means of two *independent* populations
  
  - *Pooled standard deviation*: more precise estimation of the *fixed common* variance $\sigma^2$ under various populations with different means
  $$
    \hat{\sigma}^2=s_p^2 = \frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2}
  $$
  Reduces df by injecting info about group means: $(Y_1-\bar{Y},\dots,Y_n-\bar{Y})$ is Gaussian in $\Delta_{n-1}$ simplex, while $(Y_{11}-\bar{Y}_1,\dots,Y_{nn_k}-\bar{Y}_k)$ is Gaussian in the joint simplex $\Delta_{n_1-1}\times\dots\times \Delta_{n_K-1}$.
  
  - Statistic: $T=\frac{\bar{X}_1-\bar{X}_2}{s_p}$ with $\textrm{df}=n_1+n_2-2$.
  - For unequal variance, [Welch's t-test or the exact method](https://en.wikipedia.org/wiki/Student%27s_t-test#Equal_or_unequal_sample_sizes,_unequal_variances_(sX1_%3E_2sX2_or_sX2_%3E_2sX1)) may apply.

- Paired t-test: comparing means of *one population* before and after a treatment
  $$
    T = \frac{\bar{D}-\mu_D}{s_D/\sqrt{n}}
  $$
  where $\bar{D}$ and $s_D$ are the average and sample standard deviation of the difference variables $D_i = X_i^{\textrm{pre}} - X_i^\textrm{post}$.

### F-test

- F-test is a statistical test that compares whether the *variances of two populations are significantly different*.
- Usually formulated as the ratio two scaled sum of squares.
- These sums of squares are constructed so that the **statistic tends to be greater when the null hypothesis is not true**.

#### One-way Analysis of Variance (ANOVA)

Let there be $K$ groups. Each groups has $n_k$ data such that $n_1+\dots,n_K=N$. Let $Y_{ij}$, $i\in[n_j]$, $j\in[K]$, be the $i$-th data in the $j$-th group. Assume

- Residuals $Y_{ij}-\bar{Y}_j$ of each group are normal or approximately normal
- Residuals of each group have similar variance
- Data are independently sampled

Then, the $F$ statistic is constructed by
$$
  F = \frac{\textrm{between-group variance}}{\textrm{within-group variance}} = \frac{\sum_{j\in[K]}n_j(\bar{Y}_{j}-\bar{Y})^2/(K-1)}{\sum_{i\in[n_j],j\in[K]}(Y_{ij}-\bar{Y}_j)^2/(N-K)}.
$$

*Remarks*:

- $F\sim F_{(K-1,N-K)}$ when the null hypothesis --- there is no significant difference in means among groups --- is true
- *Violation of the null hypothesis increases the statistic*: the within-group variance remains the same, while the between-group variance will significantly increase.



